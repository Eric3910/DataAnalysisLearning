{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-2-651fb6cbf51c>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-651fb6cbf51c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ├── __init__.py\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "jieba\n",
    "├── __init__.py\n",
    "├── __main__.py\n",
    "├── __pycache__\n",
    "│   ├── __init__.cpython-37.pyc\n",
    "│   ├── __main__.cpython-37.pyc\n",
    "│   └── _compat.cpython-37.pyc\n",
    "├── _compat.py\n",
    "├── analyse\n",
    "│   ├── __init__.py\n",
    "│   ├── __pycache__\n",
    "│   │   ├── __init__.cpython-37.pyc\n",
    "│   │   ├── analyzer.cpython-37.pyc\n",
    "│   │   ├── textrank.cpython-37.pyc\n",
    "│   │   └── tfidf.cpython-37.pyc\n",
    "│   ├── analyzer.py\n",
    "│   ├── idf.txt\n",
    "│   ├── textrank.py\n",
    "│   └── tfidf.py\n",
    "├── dict.txt\n",
    "├── finalseg\n",
    "│   ├── __init__.py\n",
    "│   ├── __pycache__\n",
    "│   │   ├── __init__.cpython-37.pyc\n",
    "│   │   ├── prob_emit.cpython-37.pyc\n",
    "│   │   ├── prob_start.cpython-37.pyc\n",
    "│   │   └── prob_trans.cpython-37.pyc\n",
    "│   ├── prob_emit.p\n",
    "│   ├── prob_emit.py\n",
    "│   ├── prob_start.p\n",
    "│   ├── prob_start.py\n",
    "│   ├── prob_trans.p\n",
    "│   └── prob_trans.py\n",
    "└── posseg\n",
    "    ├── __init__.py\n",
    "    ├── __pycache__\n",
    "    │   ├── __init__.cpython-37.pyc\n",
    "    │   ├── char_state_tab.cpython-37.pyc\n",
    "    │   ├── prob_emit.cpython-37.pyc\n",
    "    │   ├── prob_start.cpython-37.pyc\n",
    "    │   ├── prob_trans.cpython-37.pyc\n",
    "    │   └── viterbi.cpython-37.pyc\n",
    "    ├── char_state_tab.p\n",
    "    ├── char_state_tab.py\n",
    "    ├── prob_emit.p\n",
    "    ├── prob_emit.py\n",
    "    ├── prob_start.p\n",
    "    ├── prob_start.py\n",
    "    ├── prob_trans.p\n",
    "    ├── prob_trans.py\n",
    "    └── viterbi.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '__main__._compat'; '__main__' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a498e3b8cee9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhashlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_compat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfinalseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '__main__._compat'; '__main__' is not a package"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, unicode_literals\n",
    "__version__ = '0.39'\n",
    "__license__ = 'MIT'\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import marshal\n",
    "import tempfile\n",
    "import threading\n",
    "from math import log\n",
    "from hashlib import md5\n",
    "from ._compat import *\n",
    "from . import finalseg\n",
    "\n",
    "if os.name == 'nt':\n",
    "    from shutil import move as _replace_file\n",
    "else:\n",
    "    _replace_file = os.rename\n",
    "\n",
    "_get_abs_path = lambda path: os.path.normpath(os.path.join(os.getcwd(), path))\n",
    "\n",
    "DEFAULT_DICT = None\n",
    "DEFAULT_DICT_NAME = \"dict.txt\"\n",
    "\n",
    "log_console = logging.StreamHandler(sys.stderr)\n",
    "default_logger = logging.getLogger(__name__)\n",
    "default_logger.setLevel(logging.DEBUG)\n",
    "default_logger.addHandler(log_console)\n",
    "\n",
    "DICT_WRITING = {}\n",
    "\n",
    "pool = None\n",
    "\n",
    "re_userdict = re.compile('^(.+?)( [0-9]+)?( [a-z]+)?$', re.U)\n",
    "\n",
    "re_eng = re.compile('[a-zA-Z0-9]', re.U)\n",
    "\n",
    "# \\u4E00-\\u9FD5a-zA-Z0-9+#&\\._ : All non-space characters. Will be handled with re_han\n",
    "# \\r\\n|\\s : whitespace characters. Will not be handled.\n",
    "re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%]+)\", re.U)\n",
    "re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
    "re_han_cut_all = re.compile(\"([\\u4E00-\\u9FD5]+)\", re.U)\n",
    "re_skip_cut_all = re.compile(\"[^a-zA-Z0-9+#\\n]\", re.U)\n",
    "\n",
    "def setLogLevel(log_level):\n",
    "    global logger\n",
    "    default_logger.setLevel(log_level)\n",
    "\n",
    "class Tokenizer(object):\n",
    "\n",
    "    def __init__(self, dictionary=DEFAULT_DICT):\n",
    "        self.lock = threading.RLock()\n",
    "        if dictionary == DEFAULT_DICT:\n",
    "            self.dictionary = dictionary\n",
    "        else:\n",
    "            self.dictionary = _get_abs_path(dictionary)\n",
    "        self.FREQ = {}\n",
    "        self.total = 0\n",
    "        self.user_word_tag_tab = {}\n",
    "        self.initialized = False\n",
    "        self.tmp_dir = None\n",
    "        self.cache_file = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<Tokenizer dictionary=%r>' % self.dictionary\n",
    "\n",
    "    def gen_pfdict(self, f):\n",
    "        lfreq = {}\n",
    "        ltotal = 0\n",
    "        f_name = resolve_filename(f)\n",
    "        for lineno, line in enumerate(f, 1):\n",
    "            try:\n",
    "                line = line.strip().decode('utf-8')\n",
    "                word, freq = line.split(' ')[:2]\n",
    "                freq = int(freq)\n",
    "                lfreq[word] = freq\n",
    "                ltotal += freq\n",
    "                for ch in xrange(len(word)):\n",
    "                    wfrag = word[:ch + 1]\n",
    "                    if wfrag not in lfreq:\n",
    "                        lfreq[wfrag] = 0\n",
    "            except ValueError:\n",
    "                raise ValueError(\n",
    "                    'invalid dictionary entry in %s at Line %s: %s' % (f_name, lineno, line))\n",
    "        f.close()\n",
    "        return lfreq, ltotal\n",
    "\n",
    "    def initialize(self, dictionary=None):\n",
    "        if dictionary:\n",
    "            abs_path = _get_abs_path(dictionary)\n",
    "            if self.dictionary == abs_path and self.initialized:\n",
    "                return\n",
    "            else:\n",
    "                self.dictionary = abs_path\n",
    "                self.initialized = False\n",
    "        else:\n",
    "            abs_path = self.dictionary\n",
    "\n",
    "        with self.lock:\n",
    "            try:\n",
    "                with DICT_WRITING[abs_path]:\n",
    "                    pass\n",
    "            except KeyError:\n",
    "                pass\n",
    "            if self.initialized:\n",
    "                return\n",
    "\n",
    "            default_logger.debug(\"Building prefix dict from %s ...\" % (abs_path or 'the default dictionary'))\n",
    "            t1 = time.time()\n",
    "            if self.cache_file:\n",
    "                cache_file = self.cache_file\n",
    "            # default dictionary\n",
    "            elif abs_path == DEFAULT_DICT:\n",
    "                cache_file = \"jieba.cache\"\n",
    "            # custom dictionary\n",
    "            else:\n",
    "                cache_file = \"jieba.u%s.cache\" % md5(\n",
    "                    abs_path.encode('utf-8', 'replace')).hexdigest()\n",
    "            cache_file = os.path.join(\n",
    "                self.tmp_dir or tempfile.gettempdir(), cache_file)\n",
    "            # prevent absolute path in self.cache_file\n",
    "            tmpdir = os.path.dirname(cache_file)\n",
    "\n",
    "            load_from_cache_fail = True\n",
    "            if os.path.isfile(cache_file) and (abs_path == DEFAULT_DICT or\n",
    "                os.path.getmtime(cache_file) > os.path.getmtime(abs_path)):\n",
    "                default_logger.debug(\n",
    "                    \"Loading model from cache %s\" % cache_file)\n",
    "                try:\n",
    "                    with open(cache_file, 'rb') as cf:\n",
    "                        self.FREQ, self.total = marshal.load(cf)\n",
    "                    load_from_cache_fail = False\n",
    "                except Exception:\n",
    "                    load_from_cache_fail = True\n",
    "\n",
    "            if load_from_cache_fail:\n",
    "                wlock = DICT_WRITING.get(abs_path, threading.RLock())\n",
    "                DICT_WRITING[abs_path] = wlock\n",
    "                with wlock:\n",
    "                    self.FREQ, self.total = self.gen_pfdict(self.get_dict_file())\n",
    "                    default_logger.debug(\n",
    "                        \"Dumping model to file cache %s\" % cache_file)\n",
    "                    try:\n",
    "                        # prevent moving across different filesystems\n",
    "                        fd, fpath = tempfile.mkstemp(dir=tmpdir)\n",
    "                        with os.fdopen(fd, 'wb') as temp_cache_file:\n",
    "                            marshal.dump(\n",
    "                                (self.FREQ, self.total), temp_cache_file)\n",
    "                        _replace_file(fpath, cache_file)\n",
    "                    except Exception:\n",
    "                        default_logger.exception(\"Dump cache file failed.\")\n",
    "\n",
    "                try:\n",
    "                    del DICT_WRITING[abs_path]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "            self.initialized = True\n",
    "            default_logger.debug(\n",
    "                \"Loading model cost %.3f seconds.\" % (time.time() - t1))\n",
    "            default_logger.debug(\"Prefix dict has been built succesfully.\")\n",
    "\n",
    "    def check_initialized(self):\n",
    "        if not self.initialized:\n",
    "            self.initialize()\n",
    "\n",
    "    def calc(self, sentence, DAG, route):\n",
    "        N = len(sentence)\n",
    "        route[N] = (0, 0)\n",
    "        logtotal = log(self.total)\n",
    "        for idx in xrange(N - 1, -1, -1):\n",
    "            route[idx] = max((log(self.FREQ.get(sentence[idx:x + 1]) or 1) -\n",
    "                              logtotal + route[x + 1][0], x) for x in DAG[idx])\n",
    "\n",
    "    def get_DAG(self, sentence):\n",
    "        self.check_initialized()\n",
    "        DAG = {}\n",
    "        N = len(sentence)\n",
    "        for k in xrange(N):\n",
    "            tmplist = []\n",
    "            i = k\n",
    "            frag = sentence[k]\n",
    "            while i < N and frag in self.FREQ:\n",
    "                if self.FREQ[frag]:\n",
    "                    tmplist.append(i)\n",
    "                i += 1\n",
    "                frag = sentence[k:i + 1]\n",
    "            if not tmplist:\n",
    "                tmplist.append(k)\n",
    "            DAG[k] = tmplist\n",
    "        return DAG\n",
    "\n",
    "    def __cut_all(self, sentence):\n",
    "        dag = self.get_DAG(sentence)\n",
    "        old_j = -1\n",
    "        for k, L in iteritems(dag):\n",
    "            if len(L) == 1 and k > old_j:\n",
    "                yield sentence[k:L[0] + 1]\n",
    "                old_j = L[0]\n",
    "            else:\n",
    "                for j in L:\n",
    "                    if j > k:\n",
    "                        yield sentence[k:j + 1]\n",
    "                        old_j = j\n",
    "\n",
    "    def __cut_DAG_NO_HMM(self, sentence):\n",
    "        DAG = self.get_DAG(sentence)\n",
    "        route = {}\n",
    "        self.calc(sentence, DAG, route)\n",
    "        x = 0\n",
    "        N = len(sentence)\n",
    "        buf = ''\n",
    "        while x < N:\n",
    "            y = route[x][1] + 1\n",
    "            l_word = sentence[x:y]\n",
    "            if re_eng.match(l_word) and len(l_word) == 1:\n",
    "                buf += l_word\n",
    "                x = y\n",
    "            else:\n",
    "                if buf:\n",
    "                    yield buf\n",
    "                    buf = ''\n",
    "                yield l_word\n",
    "                x = y\n",
    "        if buf:\n",
    "            yield buf\n",
    "            buf = ''\n",
    "\n",
    "    def __cut_DAG(self, sentence):\n",
    "        DAG = self.get_DAG(sentence)\n",
    "        route = {}\n",
    "        self.calc(sentence, DAG, route)\n",
    "        x = 0\n",
    "        buf = ''\n",
    "        N = len(sentence)\n",
    "        while x < N:\n",
    "            y = route[x][1] + 1\n",
    "            l_word = sentence[x:y]\n",
    "            if y - x == 1:\n",
    "                buf += l_word\n",
    "            else:\n",
    "                if buf:\n",
    "                    if len(buf) == 1:\n",
    "                        yield buf\n",
    "                        buf = ''\n",
    "                    else:\n",
    "                        if not self.FREQ.get(buf):\n",
    "                            recognized = finalseg.cut(buf)\n",
    "                            for t in recognized:\n",
    "                                yield t\n",
    "                        else:\n",
    "                            for elem in buf:\n",
    "                                yield elem\n",
    "                        buf = ''\n",
    "                yield l_word\n",
    "            x = y\n",
    "\n",
    "        if buf:\n",
    "            if len(buf) == 1:\n",
    "                yield buf\n",
    "            elif not self.FREQ.get(buf):\n",
    "                recognized = finalseg.cut(buf)\n",
    "                for t in recognized:\n",
    "                    yield t\n",
    "            else:\n",
    "                for elem in buf:\n",
    "                    yield elem\n",
    "\n",
    "    def cut(self, sentence, cut_all=False, HMM=True):\n",
    "        '''\n",
    "        The main function that segments an entire sentence that contains\n",
    "        Chinese characters into seperated words.\n",
    "\n",
    "        Parameter:\n",
    "            - sentence: The str(unicode) to be segmented.\n",
    "            - cut_all: Model type. True for full pattern, False for accurate pattern.\n",
    "            - HMM: Whether to use the Hidden Markov Model.\n",
    "        '''\n",
    "        sentence = strdecode(sentence)\n",
    "\n",
    "        if cut_all:\n",
    "            re_han = re_han_cut_all\n",
    "            re_skip = re_skip_cut_all\n",
    "        else:\n",
    "            re_han = re_han_default\n",
    "            re_skip = re_skip_default\n",
    "        if cut_all:\n",
    "            cut_block = self.__cut_all\n",
    "        elif HMM:\n",
    "            cut_block = self.__cut_DAG\n",
    "        else:\n",
    "            cut_block = self.__cut_DAG_NO_HMM\n",
    "        blocks = re_han.split(sentence)\n",
    "        for blk in blocks:\n",
    "            if not blk:\n",
    "                continue\n",
    "            if re_han.match(blk):\n",
    "                for word in cut_block(blk):\n",
    "                    yield word\n",
    "            else:\n",
    "                tmp = re_skip.split(blk)\n",
    "                for x in tmp:\n",
    "                    if re_skip.match(x):\n",
    "                        yield x\n",
    "                    elif not cut_all:\n",
    "                        for xx in x:\n",
    "                            yield xx\n",
    "                    else:\n",
    "                        yield x\n",
    "\n",
    "    def cut_for_search(self, sentence, HMM=True):\n",
    "        \"\"\"\n",
    "        Finer segmentation for search engines.\n",
    "        \"\"\"\n",
    "        words = self.cut(sentence, HMM=HMM)\n",
    "        for w in words:\n",
    "            if len(w) > 2:\n",
    "                for i in xrange(len(w) - 1):\n",
    "                    gram2 = w[i:i + 2]\n",
    "                    if self.FREQ.get(gram2):\n",
    "                        yield gram2\n",
    "            if len(w) > 3:\n",
    "                for i in xrange(len(w) - 2):\n",
    "                    gram3 = w[i:i + 3]\n",
    "                    if self.FREQ.get(gram3):\n",
    "                        yield gram3\n",
    "            yield w\n",
    "\n",
    "    def lcut(self, *args, **kwargs):\n",
    "        return list(self.cut(*args, **kwargs))\n",
    "\n",
    "    def lcut_for_search(self, *args, **kwargs):\n",
    "        return list(self.cut_for_search(*args, **kwargs))\n",
    "\n",
    "    _lcut = lcut\n",
    "    _lcut_for_search = lcut_for_search\n",
    "\n",
    "    def _lcut_no_hmm(self, sentence):\n",
    "        return self.lcut(sentence, False, False)\n",
    "\n",
    "    def _lcut_all(self, sentence):\n",
    "        return self.lcut(sentence, True)\n",
    "\n",
    "    def _lcut_for_search_no_hmm(self, sentence):\n",
    "        return self.lcut_for_search(sentence, False)\n",
    "\n",
    "    def get_dict_file(self):\n",
    "        if self.dictionary == DEFAULT_DICT:\n",
    "            return get_module_res(DEFAULT_DICT_NAME)\n",
    "        else:\n",
    "            return open(self.dictionary, 'rb')\n",
    "\n",
    "    def load_userdict(self, f):\n",
    "        '''\n",
    "        Load personalized dict to improve detect rate.\n",
    "\n",
    "        Parameter:\n",
    "            - f : A plain text file contains words and their ocurrences.\n",
    "                  Can be a file-like object, or the path of the dictionary file,\n",
    "                  whose encoding must be utf-8.\n",
    "\n",
    "        Structure of dict file:\n",
    "        word1 freq1 word_type1\n",
    "        word2 freq2 word_type2\n",
    "        ...\n",
    "        Word type may be ignored\n",
    "        '''\n",
    "        self.check_initialized()\n",
    "        if isinstance(f, string_types):\n",
    "            f_name = f\n",
    "            f = open(f, 'rb')\n",
    "        else:\n",
    "            f_name = resolve_filename(f)\n",
    "        for lineno, ln in enumerate(f, 1):\n",
    "            line = ln.strip()\n",
    "            if not isinstance(line, text_type):\n",
    "                try:\n",
    "                    line = line.decode('utf-8').lstrip('\\ufeff')\n",
    "                except UnicodeDecodeError:\n",
    "                    raise ValueError('dictionary file %s must be utf-8' % f_name)\n",
    "            if not line:\n",
    "                continue\n",
    "            # match won't be None because there's at least one character\n",
    "            word, freq, tag = re_userdict.match(line).groups()\n",
    "            if freq is not None:\n",
    "                freq = freq.strip()\n",
    "            if tag is not None:\n",
    "                tag = tag.strip()\n",
    "            self.add_word(word, freq, tag)\n",
    "\n",
    "    def add_word(self, word, freq=None, tag=None):\n",
    "        \"\"\"\n",
    "        Add a word to dictionary.\n",
    "\n",
    "        freq and tag can be omitted, freq defaults to be a calculated value\n",
    "        that ensures the word can be cut out.\n",
    "        \"\"\"\n",
    "        self.check_initialized()\n",
    "        word = strdecode(word)\n",
    "        freq = int(freq) if freq is not None else self.suggest_freq(word, False)\n",
    "        self.FREQ[word] = freq\n",
    "        self.total += freq\n",
    "        if tag:\n",
    "            self.user_word_tag_tab[word] = tag\n",
    "        for ch in xrange(len(word)):\n",
    "            wfrag = word[:ch + 1]\n",
    "            if wfrag not in self.FREQ:\n",
    "                self.FREQ[wfrag] = 0\n",
    "        if freq == 0:\n",
    "            finalseg.add_force_split(word)\n",
    "\n",
    "    def del_word(self, word):\n",
    "        \"\"\"\n",
    "        Convenient function for deleting a word.\n",
    "        \"\"\"\n",
    "        self.add_word(word, 0)\n",
    "\n",
    "    def suggest_freq(self, segment, tune=False):\n",
    "        \"\"\"\n",
    "        Suggest word frequency to force the characters in a word to be\n",
    "        joined or splitted.\n",
    "\n",
    "        Parameter:\n",
    "            - segment : The segments that the word is expected to be cut into,\n",
    "                        If the word should be treated as a whole, use a str.\n",
    "            - tune : If True, tune the word frequency.\n",
    "\n",
    "        Note that HMM may affect the final result. If the result doesn't change,\n",
    "        set HMM=False.\n",
    "        \"\"\"\n",
    "        self.check_initialized()\n",
    "        ftotal = float(self.total)\n",
    "        freq = 1\n",
    "        if isinstance(segment, string_types):\n",
    "            word = segment\n",
    "            for seg in self.cut(word, HMM=False):\n",
    "                freq *= self.FREQ.get(seg, 1) / ftotal\n",
    "            freq = max(int(freq * self.total) + 1, self.FREQ.get(word, 1))\n",
    "        else:\n",
    "            segment = tuple(map(strdecode, segment))\n",
    "            word = ''.join(segment)\n",
    "            for seg in segment:\n",
    "                freq *= self.FREQ.get(seg, 1) / ftotal\n",
    "            freq = min(int(freq * self.total), self.FREQ.get(word, 0))\n",
    "        if tune:\n",
    "            add_word(word, freq)\n",
    "        return freq\n",
    "\n",
    "    def tokenize(self, unicode_sentence, mode=\"default\", HMM=True):\n",
    "        \"\"\"\n",
    "        Tokenize a sentence and yields tuples of (word, start, end)\n",
    "\n",
    "        Parameter:\n",
    "            - sentence: the str(unicode) to be segmented.\n",
    "            - mode: \"default\" or \"search\", \"search\" is for finer segmentation.\n",
    "            - HMM: whether to use the Hidden Markov Model.\n",
    "        \"\"\"\n",
    "        if not isinstance(unicode_sentence, text_type):\n",
    "            raise ValueError(\"jieba: the input parameter should be unicode.\")\n",
    "        start = 0\n",
    "        if mode == 'default':\n",
    "            for w in self.cut(unicode_sentence, HMM=HMM):\n",
    "                width = len(w)\n",
    "                yield (w, start, start + width)\n",
    "                start += width\n",
    "        else:\n",
    "            for w in self.cut(unicode_sentence, HMM=HMM):\n",
    "                width = len(w)\n",
    "                if len(w) > 2:\n",
    "                    for i in xrange(len(w) - 1):\n",
    "                        gram2 = w[i:i + 2]\n",
    "                        if self.FREQ.get(gram2):\n",
    "                            yield (gram2, start + i, start + i + 2)\n",
    "                if len(w) > 3:\n",
    "                    for i in xrange(len(w) - 2):\n",
    "                        gram3 = w[i:i + 3]\n",
    "                        if self.FREQ.get(gram3):\n",
    "                            yield (gram3, start + i, start + i + 3)\n",
    "                yield (w, start, start + width)\n",
    "                start += width\n",
    "\n",
    "    def set_dictionary(self, dictionary_path):\n",
    "        with self.lock:\n",
    "            abs_path = _get_abs_path(dictionary_path)\n",
    "            if not os.path.isfile(abs_path):\n",
    "                raise Exception(\"jieba: file does not exist: \" + abs_path)\n",
    "            self.dictionary = abs_path\n",
    "            self.initialized = False\n",
    "\n",
    "\n",
    "# default Tokenizer instance\n",
    "\n",
    "dt = Tokenizer()\n",
    "\n",
    "# global functions\n",
    "\n",
    "get_FREQ = lambda k, d=None: dt.FREQ.get(k, d)\n",
    "add_word = dt.add_word\n",
    "calc = dt.calc\n",
    "cut = dt.cut\n",
    "lcut = dt.lcut\n",
    "cut_for_search = dt.cut_for_search\n",
    "lcut_for_search = dt.lcut_for_search\n",
    "del_word = dt.del_word\n",
    "get_DAG = dt.get_DAG\n",
    "get_dict_file = dt.get_dict_file\n",
    "initialize = dt.initialize\n",
    "load_userdict = dt.load_userdict\n",
    "set_dictionary = dt.set_dictionary\n",
    "suggest_freq = dt.suggest_freq\n",
    "tokenize = dt.tokenize\n",
    "user_word_tag_tab = dt.user_word_tag_tab\n",
    "\n",
    "\n",
    "def _lcut_all(s):\n",
    "    return dt._lcut_all(s)\n",
    "\n",
    "\n",
    "def _lcut(s):\n",
    "    return dt._lcut(s)\n",
    "\n",
    "\n",
    "def _lcut_no_hmm(s):\n",
    "    return dt._lcut_no_hmm(s)\n",
    "\n",
    "\n",
    "def _lcut_all(s):\n",
    "    return dt._lcut_all(s)\n",
    "\n",
    "\n",
    "def _lcut_for_search(s):\n",
    "    return dt._lcut_for_search(s)\n",
    "\n",
    "\n",
    "def _lcut_for_search_no_hmm(s):\n",
    "    return dt._lcut_for_search_no_hmm(s)\n",
    "\n",
    "\n",
    "def _pcut(sentence, cut_all=False, HMM=True):\n",
    "    parts = strdecode(sentence).splitlines(True)\n",
    "    if cut_all:\n",
    "        result = pool.map(_lcut_all, parts)\n",
    "    elif HMM:\n",
    "        result = pool.map(_lcut, parts)\n",
    "    else:\n",
    "        result = pool.map(_lcut_no_hmm, parts)\n",
    "    for r in result:\n",
    "        for w in r:\n",
    "            yield w\n",
    "\n",
    "\n",
    "def _pcut_for_search(sentence, HMM=True):\n",
    "    parts = strdecode(sentence).splitlines(True)\n",
    "    if HMM:\n",
    "        result = pool.map(_lcut_for_search, parts)\n",
    "    else:\n",
    "        result = pool.map(_lcut_for_search_no_hmm, parts)\n",
    "    for r in result:\n",
    "        for w in r:\n",
    "            yield w\n",
    "\n",
    "\n",
    "def enable_parallel(processnum=None):\n",
    "    \"\"\"\n",
    "    Change the module's `cut` and `cut_for_search` functions to the\n",
    "    parallel version.\n",
    "\n",
    "    Note that this only works using dt, custom Tokenizer\n",
    "    instances are not supported.\n",
    "    \"\"\"\n",
    "    global pool, dt, cut, cut_for_search\n",
    "    from multiprocessing import cpu_count\n",
    "    if os.name == 'nt':\n",
    "        raise NotImplementedError(\n",
    "            \"jieba: parallel mode only supports posix system\")\n",
    "    else:\n",
    "        from multiprocessing import Pool\n",
    "    dt.check_initialized()\n",
    "    if processnum is None:\n",
    "        processnum = cpu_count()\n",
    "    pool = Pool(processnum)\n",
    "    cut = _pcut\n",
    "    cut_for_search = _pcut_for_search\n",
    "\n",
    "\n",
    "def disable_parallel():\n",
    "    global pool, dt, cut, cut_for_search\n",
    "    if pool:\n",
    "        pool.close()\n",
    "        pool = None\n",
    "    cut = dt.cut\n",
    "    cut_for_search = dt.cut_for_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "__main.py__\n",
    "Jieba command line interface.\"\"\"\n",
    "import sys\n",
    "import jieba\n",
    "from argparse import ArgumentParser\n",
    "from ._compat import *\n",
    "\n",
    "parser = ArgumentParser(usage=\"%s -m jieba [options] filename\" % sys.executable, description=\"Jieba command line interface.\", epilog=\"If no filename specified, use STDIN instead.\")\n",
    "parser.add_argument(\"-d\", \"--delimiter\", metavar=\"DELIM\", default=' / ',\n",
    "                    nargs='?', const=' ',\n",
    "                    help=\"use DELIM instead of ' / ' for word delimiter; or a space if it is used without DELIM\")\n",
    "parser.add_argument(\"-p\", \"--pos\", metavar=\"DELIM\", nargs='?', const='_',\n",
    "                    help=\"enable POS tagging; if DELIM is specified, use DELIM instead of '_' for POS delimiter\")\n",
    "parser.add_argument(\"-D\", \"--dict\", help=\"use DICT as dictionary\")\n",
    "parser.add_argument(\"-u\", \"--user-dict\",\n",
    "                    help=\"use USER_DICT together with the default dictionary or DICT (if specified)\")\n",
    "parser.add_argument(\"-a\", \"--cut-all\",\n",
    "                    action=\"store_true\", dest=\"cutall\", default=False,\n",
    "                    help=\"full pattern cutting (ignored with POS tagging)\")\n",
    "parser.add_argument(\"-n\", \"--no-hmm\", dest=\"hmm\", action=\"store_false\",\n",
    "                    default=True, help=\"don't use the Hidden Markov Model\")\n",
    "parser.add_argument(\"-q\", \"--quiet\", action=\"store_true\", default=False,\n",
    "                    help=\"don't print loading messages to stderr\")\n",
    "parser.add_argument(\"-V\", '--version', action='version',\n",
    "                    version=\"Jieba \" + jieba.__version__)\n",
    "parser.add_argument(\"filename\", nargs='?', help=\"input file\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.quiet:\n",
    "    jieba.setLogLevel(60)\n",
    "if args.pos:\n",
    "    import jieba.posseg\n",
    "    posdelim = args.pos\n",
    "    def cutfunc(sentence, _, HMM=True):\n",
    "        for w, f in jieba.posseg.cut(sentence, HMM):\n",
    "            yield w + posdelim + f\n",
    "else:\n",
    "    cutfunc = jieba.cut\n",
    "\n",
    "delim = text_type(args.delimiter)\n",
    "cutall = args.cutall\n",
    "hmm = args.hmm\n",
    "fp = open(args.filename, 'r') if args.filename else sys.stdin\n",
    "\n",
    "if args.dict:\n",
    "    jieba.initialize(args.dict)\n",
    "else:\n",
    "    jieba.initialize()\n",
    "if args.user_dict:\n",
    "    jieba.load_userdict(args.user_dict)\n",
    "\n",
    "ln = fp.readline()\n",
    "while ln:\n",
    "    l = ln.rstrip('\\r\\n')\n",
    "    result = delim.join(cutfunc(ln.rstrip('\\r\\n'), cutall, hmm))\n",
    "    if PY2:\n",
    "        result = result.encode(default_encoding)\n",
    "    print(result)\n",
    "    ln = fp.readline()\n",
    "\n",
    "fp.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
